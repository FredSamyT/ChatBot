{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP5RRMRUI/KaBzAuX2dYIGb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d82db55-b534-4e00-d5a5-e12cbcd175ff","id":"m_KcJiSPnJGw"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Model: \"sequential_20\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_57 (Dense)            (None, 128)               7552      \n","                                                                 \n"," dropout_38 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_58 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dropout_39 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_59 (Dense)            (None, 16)                1040      \n","                                                                 \n","=================================================================\n","Total params: 16848 (65.81 KB)\n","Trainable params: 16848 (65.81 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/150\n","2/2 [==============================] - 1s 10ms/step - loss: 2.7585 - accuracy: 0.0976\n","Epoch 2/150\n","2/2 [==============================] - 0s 8ms/step - loss: 2.6185 - accuracy: 0.1707\n","Epoch 3/150\n","2/2 [==============================] - 0s 10ms/step - loss: 2.4621 - accuracy: 0.2683\n","Epoch 4/150\n","2/2 [==============================] - 0s 9ms/step - loss: 2.3439 - accuracy: 0.2439\n","Epoch 5/150\n","2/2 [==============================] - 0s 9ms/step - loss: 2.1574 - accuracy: 0.2683\n","Epoch 6/150\n","2/2 [==============================] - 0s 9ms/step - loss: 1.8926 - accuracy: 0.4146\n","Epoch 7/150\n","2/2 [==============================] - 0s 9ms/step - loss: 1.9911 - accuracy: 0.3415\n","Epoch 8/150\n","2/2 [==============================] - 0s 9ms/step - loss: 1.7481 - accuracy: 0.3902\n","Epoch 9/150\n","2/2 [==============================] - 0s 9ms/step - loss: 1.6957 - accuracy: 0.4878\n","Epoch 10/150\n","2/2 [==============================] - 0s 15ms/step - loss: 1.4360 - accuracy: 0.5610\n","Epoch 11/150\n","2/2 [==============================] - 0s 9ms/step - loss: 1.2340 - accuracy: 0.6341\n","Epoch 12/150\n","2/2 [==============================] - 0s 10ms/step - loss: 1.3083 - accuracy: 0.6341\n","Epoch 13/150\n","2/2 [==============================] - 0s 10ms/step - loss: 1.0482 - accuracy: 0.6341\n","Epoch 14/150\n","2/2 [==============================] - 0s 9ms/step - loss: 1.1095 - accuracy: 0.7073\n","Epoch 15/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.8986 - accuracy: 0.7561\n","Epoch 16/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.9721 - accuracy: 0.7073\n","Epoch 17/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.8014 - accuracy: 0.8293\n","Epoch 18/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.7619 - accuracy: 0.7561\n","Epoch 19/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.7972 - accuracy: 0.7317\n","Epoch 20/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.6666 - accuracy: 0.8293\n","Epoch 21/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.6198 - accuracy: 0.7805\n","Epoch 22/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.3305 - accuracy: 0.8780\n","Epoch 23/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.5874 - accuracy: 0.7805\n","Epoch 24/150\n","2/2 [==============================] - 0s 13ms/step - loss: 0.4592 - accuracy: 0.8537\n","Epoch 25/150\n","2/2 [==============================] - 0s 13ms/step - loss: 0.4942 - accuracy: 0.8293\n","Epoch 26/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.5774 - accuracy: 0.8049\n","Epoch 27/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.3986 - accuracy: 0.9268\n","Epoch 28/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.3618 - accuracy: 0.9268\n","Epoch 29/150\n","2/2 [==============================] - 0s 13ms/step - loss: 0.4952 - accuracy: 0.7805\n","Epoch 30/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.3615 - accuracy: 0.8780\n","Epoch 31/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.4101 - accuracy: 0.8780\n","Epoch 32/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.3817 - accuracy: 0.8780\n","Epoch 33/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2954 - accuracy: 0.8780\n","Epoch 34/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.2120 - accuracy: 0.9512\n","Epoch 35/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2300 - accuracy: 0.9512\n","Epoch 36/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.5119 - accuracy: 0.7805\n","Epoch 37/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2226 - accuracy: 0.9512\n","Epoch 38/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.3859 - accuracy: 0.8293\n","Epoch 39/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2008 - accuracy: 0.9268\n","Epoch 40/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1789 - accuracy: 0.9756\n","Epoch 41/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1702 - accuracy: 0.9756\n","Epoch 42/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2321 - accuracy: 0.9024\n","Epoch 43/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.3689 - accuracy: 0.8780\n","Epoch 44/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2477 - accuracy: 0.9024\n","Epoch 45/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2553 - accuracy: 0.9268\n","Epoch 46/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1178 - accuracy: 0.9756\n","Epoch 47/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1280 - accuracy: 0.9756\n","Epoch 48/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.1773 - accuracy: 0.9268\n","Epoch 49/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.3065 - accuracy: 0.9024\n","Epoch 50/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2734 - accuracy: 0.9024\n","Epoch 51/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2224 - accuracy: 0.8780\n","Epoch 52/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2904 - accuracy: 0.9024\n","Epoch 53/150\n","2/2 [==============================] - 0s 13ms/step - loss: 0.2436 - accuracy: 0.9024\n","Epoch 54/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.3420 - accuracy: 0.9024\n","Epoch 55/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.1378 - accuracy: 0.9512\n","Epoch 56/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1776 - accuracy: 0.9512\n","Epoch 57/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.3771 - accuracy: 0.9268\n","Epoch 58/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1603 - accuracy: 0.9512\n","Epoch 59/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.1622 - accuracy: 0.9024\n","Epoch 60/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.1247 - accuracy: 0.9512\n","Epoch 61/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1282 - accuracy: 0.9512\n","Epoch 62/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2332 - accuracy: 0.9024\n","Epoch 63/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2483 - accuracy: 0.9268\n","Epoch 64/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1184 - accuracy: 0.9756\n","Epoch 65/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0933 - accuracy: 0.9756\n","Epoch 66/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1944 - accuracy: 0.9512\n","Epoch 67/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1972 - accuracy: 0.9512\n","Epoch 68/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1212 - accuracy: 0.9512\n","Epoch 69/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.0601 - accuracy: 1.0000\n","Epoch 70/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.0780 - accuracy: 0.9756\n","Epoch 71/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.1331 - accuracy: 0.9512\n","Epoch 72/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2076 - accuracy: 0.9024\n","Epoch 73/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1515 - accuracy: 0.9268\n","Epoch 74/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.2144 - accuracy: 0.9268\n","Epoch 75/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1104 - accuracy: 0.9756\n","Epoch 76/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.1086 - accuracy: 0.9512\n","Epoch 77/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.1448 - accuracy: 0.9756\n","Epoch 78/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1181 - accuracy: 0.9756\n","Epoch 79/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1457 - accuracy: 0.9756\n","Epoch 80/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0576 - accuracy: 1.0000\n","Epoch 81/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1356 - accuracy: 0.9268\n","Epoch 82/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1114 - accuracy: 0.9756\n","Epoch 83/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.1998 - accuracy: 0.9268\n","Epoch 84/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.3005 - accuracy: 0.9024\n","Epoch 85/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1710 - accuracy: 0.9268\n","Epoch 86/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2150 - accuracy: 0.9268\n","Epoch 87/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1340 - accuracy: 0.9512\n","Epoch 88/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1132 - accuracy: 0.9756\n","Epoch 89/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1506 - accuracy: 0.9512\n","Epoch 90/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2024 - accuracy: 0.8780\n","Epoch 91/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1132 - accuracy: 0.9756\n","Epoch 92/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.1003 - accuracy: 0.9512\n","Epoch 93/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.1159 - accuracy: 0.9512\n","Epoch 94/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0787 - accuracy: 0.9512\n","Epoch 95/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1243 - accuracy: 0.9512\n","Epoch 96/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2618 - accuracy: 0.9024\n","Epoch 97/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0456 - accuracy: 0.9756\n","Epoch 98/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0545 - accuracy: 0.9512\n","Epoch 99/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0994 - accuracy: 0.9512\n","Epoch 100/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2150 - accuracy: 0.9268\n","Epoch 101/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1373 - accuracy: 0.9268\n","Epoch 102/150\n","2/2 [==============================] - 0s 12ms/step - loss: 0.1698 - accuracy: 0.9512\n","Epoch 103/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1684 - accuracy: 0.9268\n","Epoch 104/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0597 - accuracy: 0.9756\n","Epoch 105/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1076 - accuracy: 0.9512\n","Epoch 106/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0578 - accuracy: 1.0000\n","Epoch 107/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0882 - accuracy: 0.9512\n","Epoch 108/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1587 - accuracy: 0.9512\n","Epoch 109/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0676 - accuracy: 0.9756\n","Epoch 110/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1675 - accuracy: 0.9512\n","Epoch 111/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1015 - accuracy: 0.9512\n","Epoch 112/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0668 - accuracy: 0.9756\n","Epoch 113/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1498 - accuracy: 0.9024\n","Epoch 114/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.1469 - accuracy: 0.9512\n","Epoch 115/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0901 - accuracy: 1.0000\n","Epoch 116/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1110 - accuracy: 0.9512\n","Epoch 117/150\n","2/2 [==============================] - 0s 8ms/step - loss: 0.0953 - accuracy: 0.9756\n","Epoch 118/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.0421 - accuracy: 1.0000\n","Epoch 119/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0155 - accuracy: 1.0000\n","Epoch 120/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0676 - accuracy: 0.9512\n","Epoch 121/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1275 - accuracy: 0.9512\n","Epoch 122/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0748 - accuracy: 0.9756\n","Epoch 123/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.1641 - accuracy: 0.9512\n","Epoch 124/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2929 - accuracy: 0.9268\n","Epoch 125/150\n","2/2 [==============================] - 0s 11ms/step - loss: 0.1222 - accuracy: 0.9756\n","Epoch 126/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0934 - accuracy: 0.9756\n","Epoch 127/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1547 - accuracy: 0.9268\n","Epoch 128/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0764 - accuracy: 0.9756\n","Epoch 129/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.2123 - accuracy: 0.9512\n","Epoch 130/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0529 - accuracy: 0.9756\n","Epoch 131/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2321 - accuracy: 0.8780\n","Epoch 132/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1655 - accuracy: 0.9268\n","Epoch 133/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0609 - accuracy: 0.9756\n","Epoch 134/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0206 - accuracy: 1.0000\n","Epoch 135/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1516 - accuracy: 0.9268\n","Epoch 136/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0904 - accuracy: 0.9756\n","Epoch 137/150\n","2/2 [==============================] - 0s 13ms/step - loss: 0.1078 - accuracy: 0.9512\n","Epoch 138/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0708 - accuracy: 0.9756\n","Epoch 139/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1721 - accuracy: 0.9512\n","Epoch 140/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.2334 - accuracy: 0.9024\n","Epoch 141/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0916 - accuracy: 0.9756\n","Epoch 142/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.1253 - accuracy: 0.9512\n","Epoch 143/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0686 - accuracy: 0.9756\n","Epoch 144/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0261 - accuracy: 1.0000\n","Epoch 145/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.0728 - accuracy: 0.9512\n","Epoch 146/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0835 - accuracy: 0.9756\n","Epoch 147/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.0988 - accuracy: 0.9268\n","Epoch 148/150\n","2/2 [==============================] - 0s 10ms/step - loss: 0.3444 - accuracy: 0.9024\n","Epoch 149/150\n","2/2 [==============================] - 0s 15ms/step - loss: 0.0873 - accuracy: 0.9756\n","Epoch 150/150\n","2/2 [==============================] - 0s 9ms/step - loss: 0.1693 - accuracy: 0.9512\n","Press '0' if you don't want to chat with this ChatBot.\n","1/1 [==============================] - 0s 64ms/step\n","Hi!\n","1/1 [==============================] - 0s 21ms/step\n","Sorry! I don't understand.\n","1/1 [==============================] - 0s 23ms/step\n","Hey!\n","1/1 [==============================] - 0s 22ms/step\n","I can do a lot of things but here are some of my skills, you can ask me: the capital of a country, its currency and its area. A random number. To calculate a math operation.\n","1/1 [==============================] - 0s 20ms/step\n","Hi!\n","1/1 [==============================] - 0s 22ms/step\n","Hello\n"]}],"source":["#Mounting the google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","data_root = '/content/drive/My Drive/MyChatBot'\n","\n","#Importing Libraries\n","\n","import json\n","import string\n","import random\n","\n","import nltk\n","import numpy as np\n","from nltk.stem import WordNetLemmatizer\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","nltk.download(\"punkt\")\n","nltk.download(\"wordnet\")\n","\n","#loading the intents.json dataset\n","data_file_path = data_root + '/intents.json'\n","data = json.loads(open(data_file_path).read())\n","\n","#Creating DataX and DataY\n","words = []#For bow model/vocabulary for pattern\n","classes = []#For bow /vocabulary for tags\n","data_x = [] #for storing each pattern\n","data_y = [] #for storing each tag corresponding to each pattern in data_x\n","\n","#Iterating over all the intents\n","for intent in data[\"intents\"]:\n","  for pattern in intent[\"patterns\"]:\n","    tokens = nltk.word_tokenize(pattern) #tokenize each pattern\n","    words.extend(tokens) #and append tokens to words\n","    data_x.append(pattern) #appending pattern to data_x\n","    data_y.append(intent[\"tag\"]), #appending the associated tag to each pattern\n","\n","  #Adding the tag to the classes if it's not there already\n","  if intent[\"tag\"] not in classes:\n","    classes.append(intent[\"tag\"])\n","\n","#Initializing lemmatizer ot get stem of words\n","lemmatizer = WordNetLemmatizer()\n","\n","#Lemmatize all the words in the vocab and convert them to lowercase\n","\n","#If the words don't appear in punctuation\n","words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n","\n","#sorting\n","words = sorted(set(words))\n","classes = sorted(set(classes))\n","\n","#text to numbers\n","training = []\n","out_empty = [0] * len(classes)\n","# creating the bag of words model\n","for idx, doc in enumerate(data_x):\n","  bow = []\n","  text = lemmatizer. lemmatize(doc.lower())\n","  for word in words:\n","    bow.append(1) if word in text else bow.append(0)\n","  # mark the index of class that the current pattern is associated\n","  # to\n","  output_row = list(out_empty)\n","  output_row[classes.index(data_y[idx])] = 1\n","  # add the one hot encoded BoW and associated classes to training\n","  training.append([bow, output_row])\n","# shuffle the data and convert it to an array\n","random.shuffle(training)\n","training = np.array(training, dtype=object)\n","# split the features and target labels\n","train_x = np.array(list(training[:, 0]))\n","train_y = np.array(list(training[:, 1]))\n","\n","#Neural network model\n","model = Sequential()\n","model.add(Dense(128, input_shape=(len(train_x[0]), ), activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation=\"relu\"))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_y[0]), activation = \"softmax\"))\n","adam = tf.keras.optimizers.legacy.Adam(learning_rate=0.01, decay=1e-6)\n","model.compile(loss='categorical_crossentropy',\n","             optimizer=adam,\n","             metrics=[\"accuracy\"])\n","print(model.summary())\n","model.fit(x=train_x, y=train_y, epochs=150, verbose=1)\n","\n","#Processing Input\n","def clean_text(text):\n","  tokens = nltk.word_tokenize(text)\n","  tokens = [lemmatizer. lemmatize(word) for word in tokens]\n","  return tokens\n","\n","def bag_of_words(text, vocab):\n","  tokens = clean_text(text)\n","  bow = [0] * len(vocab)\n","  for w in tokens:\n","    for idx, word in enumerate(vocab):\n","      if word == w:\n","        bow[idx] = 1\n","  return np.array(bow)\n","\n","def pred_class(text, vocab, labels):\n","  bow = bag_of_words(text, vocab)\n","  result = model.predict(np.array([bow]))[0] #Extracting probabilities\n","  thresh = 0.5\n","  y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n","  y_pred.sort(key=lambda x: x[1], reverse=True) #sorting by values of probability in decreasing order\n","  return_list = []\n","  for r in y_pred:\n","    return_list.append(labels[r[0]]) #Contains labels(tags) for highest probability\n","  return return_list\n","\n","def get_response(intents_list, intents_json):\n","  if len(intents_list) == 0:\n","    result = \"Sorry! I don't understand.\"\n","  else:\n","    tag = intents_list[0]\n","    list_of_intents = intents_json[\"intents\"]\n","    for i in list_of_intents:\n","      if i[\"tag\"] == tag:\n","        result = random.choice(i[\"responses\"])\n","        break\n","  return result\n","\n","#Interacting with ChatBot\n","print(\"Press '0' if you don't want to chat with this ChatBot.\")\n","while True:\n","  message = input(\"\")\n","  if message == \"0\":\n","    break\n","  intents = pred_class(message, words, classes)\n","  result = get_response(intents, data)\n","  print(result)"]},{"cell_type":"code","source":[],"metadata":{"id":"g-8T1jrWwX84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"26tAVV5SwYL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j1fJXiSswY0L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1cgoHg8-u8_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WcX7t3Cxu9Xk"},"execution_count":null,"outputs":[]}]}